{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax exercise\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](http://vision.stanford.edu/teaching/cs231n/assignments.html) on the course website.*\n",
    "\n",
    "This exercise is analogous to the SVM exercise. You will:\n",
    "\n",
    "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
    "- implement the fully-vectorized expression for its **analytic gradient**\n",
    "- **check your implementation** with numerical gradient\n",
    "- use a validation set to **tune the learning rate and regularization** strength\n",
    "- **optimize** the loss function with **SGD**\n",
    "- **visualize** the final learned weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3073)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3073)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3073)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3073)\n",
      "dev labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the linear classifier. These are the same steps as we used for the\n",
    "    SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "    \n",
    "    # subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "    X_dev = X_train[mask]\n",
    "    y_dev = y_train[mask]\n",
    "    \n",
    "    # Preprocessing: reshape the image data into rows\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "    \n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis = 0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "    X_dev -= mean_image\n",
    "    \n",
    "    # add bias dimension and transform into columns\n",
    "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)\n",
    "print('dev labels shape: ', y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier\n",
    "\n",
    "Your code for this section will all be written inside **cs231n/classifiers/softmax.py**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.324004\n",
      "sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "# First implement the naive softmax loss function with nested loops.\n",
    "# Open the file cs231n/classifiers/softmax.py and implement the\n",
    "# softmax_loss_naive function.\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_naive\n",
    "import time\n",
    "\n",
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print('loss: %f' % loss)\n",
    "print('sanity check: %f' % (-np.log(0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inline Question 1:\n",
    "Why do we expect our loss to be close to -log(0.1)? Explain briefly.**\n",
    "\n",
    "**Your answer:** *Fill this in*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: 3.051726 analytic: 3.051726, relative error: 2.093852e-08\n",
      "numerical: 0.877431 analytic: 0.877431, relative error: 3.219963e-08\n",
      "numerical: 2.214516 analytic: 2.214515, relative error: 3.677909e-08\n",
      "numerical: -1.230229 analytic: -1.230229, relative error: 1.458686e-08\n",
      "numerical: -1.109690 analytic: -1.109690, relative error: 4.656267e-08\n",
      "numerical: 2.827766 analytic: 2.827766, relative error: 3.341872e-08\n",
      "numerical: -1.901584 analytic: -1.901584, relative error: 7.344943e-08\n",
      "numerical: 2.771631 analytic: 2.771631, relative error: 2.785702e-08\n",
      "numerical: -0.961256 analytic: -0.961257, relative error: 4.123175e-08\n",
      "numerical: -1.654919 analytic: -1.654919, relative error: 1.853634e-08\n",
      "numerical: -1.382953 analytic: -1.382953, relative error: 4.333954e-08\n",
      "numerical: 0.477616 analytic: 0.477616, relative error: 3.340021e-08\n",
      "numerical: -0.497216 analytic: -0.497216, relative error: 8.926618e-09\n",
      "numerical: 0.875571 analytic: 0.875571, relative error: 9.610995e-08\n",
      "numerical: 1.405971 analytic: 1.405971, relative error: 4.358284e-08\n",
      "numerical: 0.953168 analytic: 0.953168, relative error: 1.744931e-08\n",
      "numerical: 0.676639 analytic: 0.676639, relative error: 1.106033e-07\n",
      "numerical: 0.085818 analytic: 0.085818, relative error: 3.001446e-07\n",
      "numerical: 1.334673 analytic: 1.334673, relative error: 8.884511e-08\n",
      "numerical: 1.299855 analytic: 1.299855, relative error: 9.344098e-08\n"
     ]
    }
   ],
   "source": [
    "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "# version of the gradient that uses nested loops.\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As we did for the SVM, use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "from cs231n.gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
    "\n",
    "# similar to SVM case, do another gradient check with regularization\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 5e1)\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 5e1)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive loss: 2.324004e+00 computed in 0.123082s\n",
      "vectorized loss: 2.324004e+00 computed in 0.014010s\n",
      "Loss difference: 0.000000\n",
      "Gradient difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now that we have a naive implementation of the softmax loss function and its gradient,\n",
    "# implement a vectorized version in softmax_loss_vectorized.\n",
    "# The two versions should compute the same results, but the vectorized version should be\n",
    "# much faster.\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_vectorized\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
    "\n",
    "# As we did for the SVM, we use the Frobenius norm to compare the two versions\n",
    "# of the gradient.\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n",
    "print('Gradient difference: %f' % grad_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ss\\Desktop\\spring1617_assignment1\\assignment1\\cs231n\\classifiers\\softmax.py:93: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = np.sum(margin)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 1.932128e-05 reg 2.379614e+03 train accuracy: 0.235551 val accuracy: 0.264000\n",
      "lr 1.932128e-05 reg 3.151641e+03 train accuracy: 0.163224 val accuracy: 0.196000\n",
      "lr 1.932128e-05 reg 4.299755e+03 train accuracy: 0.154082 val accuracy: 0.157000\n",
      "lr 1.932128e-05 reg 5.146865e+03 train accuracy: 0.181531 val accuracy: 0.185000\n",
      "lr 1.932128e-05 reg 5.162118e+03 train accuracy: 0.214061 val accuracy: 0.208000\n",
      "lr 1.932128e-05 reg 5.413029e+03 train accuracy: 0.200490 val accuracy: 0.210000\n",
      "lr 1.932128e-05 reg 6.840275e+03 train accuracy: 0.138653 val accuracy: 0.136000\n",
      "lr 1.932128e-05 reg 7.938290e+03 train accuracy: 0.162776 val accuracy: 0.169000\n",
      "lr 1.932128e-05 reg 8.180278e+03 train accuracy: 0.163449 val accuracy: 0.136000\n",
      "lr 1.932128e-05 reg 8.246017e+03 train accuracy: 0.183816 val accuracy: 0.196000\n",
      "lr 1.952502e-05 reg 2.379614e+03 train accuracy: 0.202796 val accuracy: 0.228000\n",
      "lr 1.952502e-05 reg 3.151641e+03 train accuracy: 0.174388 val accuracy: 0.155000\n",
      "lr 1.952502e-05 reg 4.299755e+03 train accuracy: 0.218224 val accuracy: 0.211000\n",
      "lr 1.952502e-05 reg 5.146865e+03 train accuracy: 0.170163 val accuracy: 0.168000\n",
      "lr 1.952502e-05 reg 5.162118e+03 train accuracy: 0.196980 val accuracy: 0.213000\n",
      "lr 1.952502e-05 reg 5.413029e+03 train accuracy: 0.167102 val accuracy: 0.183000\n",
      "lr 1.952502e-05 reg 6.840275e+03 train accuracy: 0.132980 val accuracy: 0.126000\n",
      "lr 1.952502e-05 reg 7.938290e+03 train accuracy: 0.159735 val accuracy: 0.171000\n",
      "lr 1.952502e-05 reg 8.180278e+03 train accuracy: 0.154571 val accuracy: 0.177000\n",
      "lr 1.952502e-05 reg 8.246017e+03 train accuracy: 0.177531 val accuracy: 0.175000\n",
      "lr 2.133765e-05 reg 2.379614e+03 train accuracy: 0.163837 val accuracy: 0.170000\n",
      "lr 2.133765e-05 reg 3.151641e+03 train accuracy: 0.219918 val accuracy: 0.215000\n",
      "lr 2.133765e-05 reg 4.299755e+03 train accuracy: 0.195571 val accuracy: 0.185000\n",
      "lr 2.133765e-05 reg 5.146865e+03 train accuracy: 0.192592 val accuracy: 0.191000\n",
      "lr 2.133765e-05 reg 5.162118e+03 train accuracy: 0.141939 val accuracy: 0.143000\n",
      "lr 2.133765e-05 reg 5.413029e+03 train accuracy: 0.183980 val accuracy: 0.174000\n",
      "lr 2.133765e-05 reg 6.840275e+03 train accuracy: 0.201204 val accuracy: 0.194000\n",
      "lr 2.133765e-05 reg 7.938290e+03 train accuracy: 0.130163 val accuracy: 0.124000\n",
      "lr 2.133765e-05 reg 8.180278e+03 train accuracy: 0.156490 val accuracy: 0.169000\n",
      "lr 2.133765e-05 reg 8.246017e+03 train accuracy: 0.151776 val accuracy: 0.147000\n",
      "lr 2.309957e-05 reg 2.379614e+03 train accuracy: 0.165571 val accuracy: 0.162000\n",
      "lr 2.309957e-05 reg 3.151641e+03 train accuracy: 0.188837 val accuracy: 0.208000\n",
      "lr 2.309957e-05 reg 4.299755e+03 train accuracy: 0.196939 val accuracy: 0.172000\n",
      "lr 2.309957e-05 reg 5.146865e+03 train accuracy: 0.154245 val accuracy: 0.156000\n",
      "lr 2.309957e-05 reg 5.162118e+03 train accuracy: 0.169469 val accuracy: 0.162000\n",
      "lr 2.309957e-05 reg 5.413029e+03 train accuracy: 0.166286 val accuracy: 0.157000\n",
      "lr 2.309957e-05 reg 6.840275e+03 train accuracy: 0.171388 val accuracy: 0.171000\n",
      "lr 2.309957e-05 reg 7.938290e+03 train accuracy: 0.130184 val accuracy: 0.126000\n",
      "lr 2.309957e-05 reg 8.180278e+03 train accuracy: 0.166939 val accuracy: 0.165000\n",
      "lr 2.309957e-05 reg 8.246017e+03 train accuracy: 0.121837 val accuracy: 0.121000\n",
      "lr 2.762555e-05 reg 2.379614e+03 train accuracy: 0.204735 val accuracy: 0.222000\n",
      "lr 2.762555e-05 reg 3.151641e+03 train accuracy: 0.148918 val accuracy: 0.143000\n",
      "lr 2.762555e-05 reg 4.299755e+03 train accuracy: 0.141653 val accuracy: 0.137000\n",
      "lr 2.762555e-05 reg 5.146865e+03 train accuracy: 0.163286 val accuracy: 0.157000\n",
      "lr 2.762555e-05 reg 5.162118e+03 train accuracy: 0.131918 val accuracy: 0.133000\n",
      "lr 2.762555e-05 reg 5.413029e+03 train accuracy: 0.143490 val accuracy: 0.143000\n",
      "lr 2.762555e-05 reg 6.840275e+03 train accuracy: 0.186000 val accuracy: 0.179000\n",
      "lr 2.762555e-05 reg 7.938290e+03 train accuracy: 0.146653 val accuracy: 0.142000\n",
      "lr 2.762555e-05 reg 8.180278e+03 train accuracy: 0.128204 val accuracy: 0.120000\n",
      "lr 2.762555e-05 reg 8.246017e+03 train accuracy: 0.115306 val accuracy: 0.108000\n",
      "lr 5.162506e-05 reg 2.379614e+03 train accuracy: 0.189531 val accuracy: 0.181000\n",
      "lr 5.162506e-05 reg 3.151641e+03 train accuracy: 0.157082 val accuracy: 0.153000\n",
      "lr 5.162506e-05 reg 4.299755e+03 train accuracy: 0.151204 val accuracy: 0.141000\n",
      "lr 5.162506e-05 reg 5.146865e+03 train accuracy: 0.144816 val accuracy: 0.147000\n",
      "lr 5.162506e-05 reg 5.162118e+03 train accuracy: 0.106857 val accuracy: 0.100000\n",
      "lr 5.162506e-05 reg 5.413029e+03 train accuracy: 0.131694 val accuracy: 0.134000\n",
      "lr 5.162506e-05 reg 6.840275e+03 train accuracy: 0.091082 val accuracy: 0.089000\n",
      "lr 5.162506e-05 reg 7.938290e+03 train accuracy: 0.110429 val accuracy: 0.122000\n",
      "lr 5.162506e-05 reg 8.180278e+03 train accuracy: 0.107633 val accuracy: 0.105000\n",
      "lr 5.162506e-05 reg 8.246017e+03 train accuracy: 0.078041 val accuracy: 0.067000\n",
      "lr 5.607095e-05 reg 2.379614e+03 train accuracy: 0.140531 val accuracy: 0.162000\n",
      "lr 5.607095e-05 reg 3.151641e+03 train accuracy: 0.152980 val accuracy: 0.180000\n",
      "lr 5.607095e-05 reg 4.299755e+03 train accuracy: 0.100612 val accuracy: 0.087000\n",
      "lr 5.607095e-05 reg 5.146865e+03 train accuracy: 0.086061 val accuracy: 0.084000\n",
      "lr 5.607095e-05 reg 5.162118e+03 train accuracy: 0.113061 val accuracy: 0.106000\n",
      "lr 5.607095e-05 reg 5.413029e+03 train accuracy: 0.113918 val accuracy: 0.115000\n",
      "lr 5.607095e-05 reg 6.840275e+03 train accuracy: 0.073776 val accuracy: 0.074000\n",
      "lr 5.607095e-05 reg 7.938290e+03 train accuracy: 0.077551 val accuracy: 0.077000\n",
      "lr 5.607095e-05 reg 8.180278e+03 train accuracy: 0.122918 val accuracy: 0.139000\n",
      "lr 5.607095e-05 reg 8.246017e+03 train accuracy: 0.133224 val accuracy: 0.134000\n",
      "lr 6.347637e-05 reg 2.379614e+03 train accuracy: 0.173388 val accuracy: 0.163000\n",
      "lr 6.347637e-05 reg 3.151641e+03 train accuracy: 0.143776 val accuracy: 0.143000\n",
      "lr 6.347637e-05 reg 4.299755e+03 train accuracy: 0.120837 val accuracy: 0.119000\n",
      "lr 6.347637e-05 reg 5.146865e+03 train accuracy: 0.082816 val accuracy: 0.080000\n",
      "lr 6.347637e-05 reg 5.162118e+03 train accuracy: 0.081469 val accuracy: 0.065000\n",
      "lr 6.347637e-05 reg 5.413029e+03 train accuracy: 0.117551 val accuracy: 0.120000\n",
      "lr 6.347637e-05 reg 6.840275e+03 train accuracy: 0.075469 val accuracy: 0.078000\n",
      "lr 6.347637e-05 reg 7.938290e+03 train accuracy: 0.074265 val accuracy: 0.069000\n",
      "lr 6.347637e-05 reg 8.180278e+03 train accuracy: 0.107224 val accuracy: 0.108000\n",
      "lr 6.347637e-05 reg 8.246017e+03 train accuracy: 0.078143 val accuracy: 0.104000\n",
      "lr 7.528032e-05 reg 2.379614e+03 train accuracy: 0.142939 val accuracy: 0.132000\n",
      "lr 7.528032e-05 reg 3.151641e+03 train accuracy: 0.147857 val accuracy: 0.153000\n",
      "lr 7.528032e-05 reg 4.299755e+03 train accuracy: 0.120122 val accuracy: 0.137000\n",
      "lr 7.528032e-05 reg 5.146865e+03 train accuracy: 0.133898 val accuracy: 0.113000\n",
      "lr 7.528032e-05 reg 5.162118e+03 train accuracy: 0.118980 val accuracy: 0.126000\n",
      "lr 7.528032e-05 reg 5.413029e+03 train accuracy: 0.077143 val accuracy: 0.077000\n",
      "lr 7.528032e-05 reg 6.840275e+03 train accuracy: 0.079306 val accuracy: 0.069000\n",
      "lr 7.528032e-05 reg 7.938290e+03 train accuracy: 0.073143 val accuracy: 0.066000\n",
      "lr 7.528032e-05 reg 8.180278e+03 train accuracy: 0.103531 val accuracy: 0.093000\n",
      "lr 7.528032e-05 reg 8.246017e+03 train accuracy: 0.138918 val accuracy: 0.148000\n",
      "lr 7.891104e-05 reg 2.379614e+03 train accuracy: 0.201082 val accuracy: 0.203000\n",
      "lr 7.891104e-05 reg 3.151641e+03 train accuracy: 0.105143 val accuracy: 0.101000\n",
      "lr 7.891104e-05 reg 4.299755e+03 train accuracy: 0.087224 val accuracy: 0.065000\n",
      "lr 7.891104e-05 reg 5.146865e+03 train accuracy: 0.070224 val accuracy: 0.064000\n",
      "lr 7.891104e-05 reg 5.162118e+03 train accuracy: 0.132694 val accuracy: 0.113000\n",
      "lr 7.891104e-05 reg 5.413029e+03 train accuracy: 0.108939 val accuracy: 0.108000\n",
      "lr 7.891104e-05 reg 6.840275e+03 train accuracy: 0.070245 val accuracy: 0.066000\n",
      "lr 7.891104e-05 reg 7.938290e+03 train accuracy: 0.082959 val accuracy: 0.088000\n",
      "lr 7.891104e-05 reg 8.180278e+03 train accuracy: 0.131796 val accuracy: 0.121000\n",
      "lr 7.891104e-05 reg 8.246017e+03 train accuracy: 0.132490 val accuracy: 0.136000\n",
      "best validation accuracy achieved during cross-validation: 0.264000\n"
     ]
    }
   ],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths; if you are careful you should be able to\n",
    "# get a classification accuracy of over 0.35 on the validation set.\n",
    "from cs231n.classifiers import Softmax\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "best_tuning = ()\n",
    "#learning_rates = [1e-7, 5e-7]\n",
    "#regularization_strengths = [2.5e4, 5e4]\n",
    "#learning_rates = [ 1e-7, 1e-6, 1e-5, 1e-4 ]\n",
    "#regularization_strengths = [1e-4, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4]\n",
    "learning_rates = np.random.uniform(1e-7, 1e-4, size=10)\n",
    "regularization_strengths = np.random.uniform(1e-6, 1e4, size=10)\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Use the validation set to set the learning rate and regularization strength. #\n",
    "# This should be identical to the validation that you did for the SVM; save    #\n",
    "# the best trained softmax classifer in best_softmax.                          #\n",
    "################################################################################\n",
    "for i in learning_rates:\n",
    "    for j in regularization_strengths:\n",
    "        softmax = Softmax()\n",
    "        softmax.train(X_train, y_train, learning_rate=i, reg=j, num_iters=200, verbose=False)\n",
    "\n",
    "        y_train_pred = softmax.predict(X_train)\n",
    "        y_val_pred = softmax.predict(X_val)\n",
    "\n",
    "        results[(i,j)] = (np.mean(y_train == y_train_pred), np.mean(y_val == y_val_pred))\n",
    "\n",
    "        if best_val < np.mean(y_val == y_val_pred):\n",
    "            best_val = np.mean(y_val == y_val_pred)\n",
    "            best_softmax = softmax\n",
    "            best_tuning = (i, j)\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################\n",
    "    \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x17301aca208>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjMAAAEDCAYAAADAwIXyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x173015a5eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "x_scatters = [math.log10(x[0]) for x in results]\n",
    "y_scatters = [math.log10(x[1]) for x in results]\n",
    "\n",
    "colors = [results[x][1] for x in results]\n",
    "plt.subplot(2,1,1)\n",
    "plt.scatter(x_scatters,y_scatters,c = colors)\n",
    "\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"learning rates\")\n",
    "plt.ylabel(\"regulazation\")\n",
    "plt.title(\"validation accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# evaluate on test set\n",
    "# Evaluate the best softmax on test set\n",
    "y_test_pred = best_softmax.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = best_softmax.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    \n",
    "    # Rescale the weights to be between 0 and 255\n",
    "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
